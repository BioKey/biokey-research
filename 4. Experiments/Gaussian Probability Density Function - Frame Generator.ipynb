{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A continuous identity verification method based on free-text keystroke dynamics](http://ieeexplore.ieee.org.proxy1.lib.uwo.ca/document/7844242/authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import credentials\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import string\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import numpy.ma as ma\n",
    "from multiprocessing import Pool\n",
    "from scipy.optimize import minimize\n",
    "from biokey.data import DataInterface\n",
    "import biokey.tools\n",
    "from biokey.tools import parallel_process\n",
    "import json\n",
    "import random\n",
    "# This makes plots render inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "\t- Attempting cache load\n",
      "\t- Loaded strokes from cache\n",
      "Processing Data\n",
      "\t- Attempting cache load\n",
      "\t- Loaded dwell and flight from cache\n",
      "Done Loading\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = DataInterface(credentials.postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_press_count = data.get_dwells().key.value_counts()\n",
    "mask = list(string.ascii_uppercase)\n",
    "mask.extend(['ESCAPE', 'ENTER', 'SPACE', 'CONTROL', 'META', 'BACKSPACE', 'SHIFT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = data.get_all_sets()\n",
    "# for u in datasets:\n",
    "#     datasets[u] = biokey.tools.filter_sets(datasets[u], to_include=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Profile Using Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind_from_stats\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.children = {}\n",
    "        self.stats = None\n",
    "        self.parent = None\n",
    "    def add(self, hash1):\n",
    "        self.children[hash1.name] = hash1\n",
    "        self.parent = self\n",
    "        return hash1\n",
    "    def goto(self, name):\n",
    "        if(name in self.children):\n",
    "            return self.children[name]\n",
    "\n",
    "class Profile(object):\n",
    "    def __init__(self, df, threshold, trim=None):\n",
    "        self.trimmed = trim\n",
    "        self.threshold = threshold\n",
    "        if data is not None and threshold is not None:\n",
    "            self._build_profile(df, threshold)\n",
    "            if trim:\n",
    "                self.profile = self.profile.loc[self.profile['count'] >= trim]\n",
    "                self._build_tree()\n",
    "    \n",
    "    def _build_profile(self, df, t):\n",
    "        df = df.copy()\n",
    "        df['interval'] = (df.shift(-1).down - df.down)\n",
    "        chain = []\n",
    "        inner_queue = []\n",
    "        start = None\n",
    "        seqs = []\n",
    "        count = -1\n",
    "        for row in df.itertuples():\n",
    "            count += 1\n",
    "            row = {\n",
    "                'key': row.key,\n",
    "                'down': row.down,\n",
    "                'up': row.up,\n",
    "                'interval': row.interval,\n",
    "                'count': count\n",
    "            }\n",
    "            if start == None:\n",
    "                # First after threshold\n",
    "                start = row\n",
    "            else:\n",
    "                # Nth after threshold - queue as starters\n",
    "                inner_queue.append(row)\n",
    "            # Add it to the chain\n",
    "            chain.append(str(row['key']))\n",
    "            seqs.append({'seq': '-'.join(chain), 'duration': row['up'] - start['down'], 'start': start['count'], 'end': row['count']})\n",
    "            # If threshold is not met then start again\n",
    "            if row['interval'] > t:\n",
    "                for i in range(len(inner_queue)):\n",
    "                    start = inner_queue[i]\n",
    "                    chain = []\n",
    "                    for r in inner_queue[i:]:\n",
    "                        chain.append(str(row['key']))\n",
    "                        seqs.append({'seq': '-'.join(chain), 'duration': row['up'] - start['down'], 'start': start['count'], 'end': row['count']})\n",
    "                chain = []\n",
    "                inner_queue = []\n",
    "                start = None\n",
    "        # Aggregate sequences\n",
    "        self.seqs = pd.DataFrame(seqs)\n",
    "        s = self.seqs.copy()\n",
    "        s.loc[:,'log_duration'] = np.log(s.duration)\n",
    "        seq_dur = s.groupby('seq').aggregate(['mean', 'std', 'count']).log_duration\n",
    "        # Fill missing values with a proxy\n",
    "        seq_dur['std'] = seq_dur['std'].fillna((seq_dur['std']/seq_dur['mean']).mean()*seq_dur['mean'])\n",
    "        self.profile = seq_dur\n",
    "        self._build_tree()\n",
    "    \n",
    "    def update_profile(self, df):\n",
    "        self.profile = df\n",
    "        self._build_tree()\n",
    "    \n",
    "    def diversify(self, other_profile):\n",
    "        compared_profiles = pd.merge(self.profile, other_profile.profile, left_index=True, right_index=True)\n",
    "        compared_profiles['pvalue'] = compared_profiles.apply((lambda x: ttest_ind_from_stats(x.mean_x, x.std_x, x.count_x, x.mean_y, x.std_y, x.count_y, False).pvalue), axis=1)\n",
    "        new_profile = compared_profiles.loc[compared_profiles.pvalue < 0.05,['mean_x', 'std_x', 'count_x']]\n",
    "        new_profile.columns = ['mean', 'std', 'count']\n",
    "        self.update_profile(new_profile)\n",
    "        \n",
    "    def _build_tree(self):\n",
    "        self.root = Node('root')\n",
    "        for s, row in self.profile.iterrows():\n",
    "            n = self.root\n",
    "            for k in s.split('-'):\n",
    "                new = n.goto(k)\n",
    "                if not new:\n",
    "                    new = n.add(Node(k))\n",
    "                n = new\n",
    "            n.stats = row.to_dict()\n",
    "            \n",
    "    def trim(self, n):\n",
    "        \"Returns a new profile, trimmed to sequences with n instances\"\n",
    "        new_p = Profile(None, self.threshold, n)\n",
    "        new_p.profile = self.profile.loc[self.profile['count'] >= n]\n",
    "        new_p.seqs = self.profile.loc[self.seqs.seq.isin(self.profile.index)]\n",
    "        new_p._build_tree()\n",
    "        return new_p\n",
    "    \n",
    "    def search_tree(self, path):\n",
    "        \"Search tree for the node at a given list path\"\n",
    "        n = self.root\n",
    "        for p in path:\n",
    "            new = n.goto(p)\n",
    "            if not new:\n",
    "                return None\n",
    "            n = new\n",
    "        return n\n",
    "    \n",
    "    def features(self):\n",
    "        return np.array(self.profile.index)\n",
    "\n",
    "    def test_distance(self, path, duration):\n",
    "        \"Returns the probability that a typed path's duration was the profiles's\"\n",
    "        node = self.search_tree(path)\n",
    "        if node is None or node.stats is None:\n",
    "            return -1\n",
    "        stats = node.stats\n",
    "        return np.exp(-np.square((np.log(duration)-stats['mean']))/(2*np.square(stats['std'])))\n",
    "        \n",
    "    def test_window(self, df):\n",
    "        df = df.copy()\n",
    "        df['interval'] = (df.shift(-1).down - df.down)\n",
    "        chain = []\n",
    "        inner_queue = []\n",
    "        start = -1\n",
    "        instances = {}\n",
    "        results = {}\n",
    "        \n",
    "        for f in self.features():\n",
    "            instances[f] = []\n",
    "            results[f] = 0\n",
    "        for row in df.itertuples():\n",
    "            if start == -1:\n",
    "                start = row.down\n",
    "            else:\n",
    "                inner_queue.append(row)\n",
    "            chain.append(str(row.key))\n",
    "            seq = '-'.join(chain)\n",
    "            if seq in instances:\n",
    "                instances[seq].append(self.test_distance(chain, row.up - start))\n",
    "            if row.interval > self.threshold:\n",
    "                for i in range(len(inner_queue)):\n",
    "                    start = inner_queue[i].down\n",
    "                    chain = []\n",
    "                    for r in inner_queue[i:]:\n",
    "                        chain.append(str(r.key))\n",
    "                        seq = '-'.join(chain)\n",
    "                        if seq in instances:\n",
    "                            instances[seq].append(self.test_distance(chain, r.up - start))\n",
    "                chain = []\n",
    "                inner_queue = []\n",
    "                start = -1\n",
    "                \n",
    "        for f in instances:\n",
    "            durations = instances[f]\n",
    "            if len(durations) > 0:\n",
    "                results[f] = np.mean(durations)\n",
    "        return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_valid = data.get_users()[2]\n",
    "train = datasets[userid_valid].train\n",
    "test = datasets[userid_valid].test\n",
    "\n",
    "train_valid = train.loc[train.is_user]\n",
    "train_imposter = train.loc[train.is_user == False]\n",
    "\n",
    "test_valid = test.loc[test.is_user]\n",
    "test_imposter = test.loc[test.is_user == False]\n",
    "\n",
    "# user_threshold = optimals[userid_valid]\n",
    "seq_cutoff_threshold = 150\n",
    "seq_occurance_threshold = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Profile(train_valid, seq_cutoff_threshold, trim=seq_occurance_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_length = 100\n",
    "\n",
    "def test_window_indexes(seqs, profile, start, end):\n",
    "    s = seqs.loc[(seqs.start >= start) & (seqs.end < end)].copy()\n",
    "    s['score'] = s.apply((lambda x: profile.test_distance(x.seq.split('-'), x.duration)), axis=1)\n",
    "    s = s.loc[s.score >= 0]\n",
    "    results = s.groupby('seq').score.aggregate(['mean', 'count']).T.to_dict()\n",
    "    '''\n",
    "    for f in profile.features():\n",
    "        if f not in results:\n",
    "            results[f] = {'count': 0, 'mean': 0}\n",
    "    '''\n",
    "    return pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 698k/698k [24:33<00:00, 474it/s]     \n",
      "698077it [00:01, 452117.90it/s]\n"
     ]
    }
   ],
   "source": [
    "seqs_all = Profile(train, seq_cutoff_threshold).seqs\n",
    "y_vals = list(train.is_user)\n",
    "\n",
    "def test_iter(i):\n",
    "    try:\n",
    "        return {\n",
    "            'x': test_window_indexes(seqs_all, p, i, i + buffer_length).T.to_dict(),\n",
    "            'y': y_vals[i+buffer_length],\n",
    "            'i': i\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# This takes a bit\n",
    "frames = parallel_process(range(buffer_length, len(y_vals)), test_iter, front_num=1, n_jobs=36)\n",
    "with open('windowed_frames_%d.json' % buffer_length, 'w') as fout:\n",
    "    json.dump([f for f in frames if f is not None], fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 727k/727k [16:50<00:00, 719it/s]     \n",
      "727322it [00:01, 438644.47it/s]\n"
     ]
    }
   ],
   "source": [
    "seqs_all_test = Profile(test, seq_cutoff_threshold).seqs\n",
    "y_vals_test = list(test.is_user)\n",
    "\n",
    "def test_iter_test(i):\n",
    "    try:\n",
    "        return {\n",
    "            'x': test_window_indexes(seqs_all_test, p, i, i + buffer_length).T.to_dict(),\n",
    "            'y': y_vals_test[i+buffer_length],\n",
    "            'i': i\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "test_frames = parallel_process(range(buffer_length, len(y_vals_test)), test_iter_test, front_num=1, n_jobs=36)\n",
    "with open('windowed_frames_test_%d.json' % buffer_length, 'w') as fout:\n",
    "    json.dump([f for f in test_frames if f is not None], fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
